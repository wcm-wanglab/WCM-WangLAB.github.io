{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262ed4fe",
   "metadata": {},
   "source": [
    "## Cohort Context Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e15151a",
   "metadata": {},
   "source": [
    "We extended our knowledge discovery pipeline for enhancing data analysis for patient cohort context exploration. This setting takes in the clinical profiles of a particular patient cohort (called the query cohort) and maps them to corresponding biomedical entities (called cohort description entities) in iBKH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419e83b",
   "metadata": {},
   "source": [
    "Please make sure all the files following the structure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0244075",
   "metadata": {},
   "source": [
    "```\n",
    ".\n",
    "└── ...\n",
    "    ├── Cohort Context Exploration.ipynb\n",
    "    ├── find_UMLS.py\n",
    "    ├── exploration_CC.py\n",
    "    ├── APOE_disease.csv\n",
    "    └── APOE_logit_reg.csv\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3636638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from Case_study.find_UMLS import *\n",
    "from Case_study.exploration_CC import *\n",
    "from neo4j import GraphDatabase\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import maxabs_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf10410",
   "metadata": {},
   "source": [
    "### Step 1: Map the AD/cognitive disorder-related traits from the APOE cohort to the entities in iBKH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73222e86",
   "metadata": {},
   "source": [
    "We collected 36 AD/cognitive disorder-related traits through a literature review. We used the UMLS API to assign corresponding UMLS CUIs for these features to map to entities in iBKH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "apoe_disease_data = pd.read_csv(\"APOE_disease.csv\")  # Read APOE cohort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the corresponding UMLS CUIs for the 36 collected traits and map them to the entiites in iBKH\n",
    "def map_entities2iBKH():\n",
    "    raw_dx_list = apoe_disease_data['Unnamed: 0'].tolist()\n",
    "    dx_cui = {}\n",
    "    for concept_name in tqdm(raw_dx_list):\n",
    "        umls_cui = access_UMLS_by_name(concept_name)\n",
    "        umls_name = get_UMLS_name(umls_cui)\n",
    "        dx_cui[concept_name] = [umls_cui, umls_name]\n",
    "\n",
    "    res = {}\n",
    "    for raw_dx in tqdm(dx_cui):\n",
    "        umls_cui, umls_name = dx_cui[raw_dx]\n",
    "        if umls_cui in di_vobUMLS_list:\n",
    "            idx = di_vob.loc[di_vob['umls_cui'] == umls_cui].index[0]\n",
    "            primary_id = di_vob.loc[di_vob['umls_cui'] == umls_cui].loc[idx, 'primary']\n",
    "            name = di_vob.loc[di_vob['umls_cui'] == umls_cui].loc[idx, 'name']\n",
    "            res[raw_dx] = [primary_id, name, 'Disease', umls_cui]\n",
    "        elif umls_cui in sy_vobUMLS_list:\n",
    "            idx = sy_vob.loc[sy_vob['umls_cui'] == umls_cui].index[0]\n",
    "            primary_id = sy_vob.loc[sy_vob['umls_cui'] == umls_cui].loc[idx, 'primary']\n",
    "            name = sy_vob.loc[sy_vob['umls_cui'] == umls_cui].loc[idx, 'name']\n",
    "            res[raw_dx] = [primary_id, name, 'Symptom', umls_cui]\n",
    "        elif umls_cui in se_vobUMLS_list:\n",
    "            idx = se_vob.loc[se_vob['umls_cui'] == umls_cui].index[0]\n",
    "            primary_id = se_vob.loc[se_vob['umls_cui'] == umls_cui].loc[idx, 'primary']\n",
    "            name = se_vob.loc[se_vob['umls_cui'] == umls_cui].loc[idx, 'name']\n",
    "            res[raw_dx] = [primary_id, name, 'Side_Effect', umls_cui]\n",
    "    print(res)\n",
    "    with open('concept_iBKH_apoe.obj', 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fff179",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_entities2iBKH()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f08e9",
   "metadata": {},
   "source": [
    "### Step 2: Generate the input list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494fff4",
   "metadata": {},
   "source": [
    "We construct the context for the APOE cohort based on the mapped result. Namely, we construct the input list to explore the APOE cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cohort_context(weight_type, pval_filter, topk):\n",
    "    raw_dx_list = apoe_disease_data['Unnamed: 0'].tolist()\n",
    "    with open(\"concept_iBKH_apoe.obj\", \"rb\") as f:\n",
    "        concept_iBKH_apoe = pickle.load(f)\n",
    "    f.close()\n",
    "    raw_dx_weight = {}\n",
    "    if weight_type == \"LR\":\n",
    "        weight_data = pd.read_csv(\"APOE_logit_reg.csv\")\n",
    "        for i in range(len(weight_data)):\n",
    "            raw_dx = weight_data.loc[i, 'Unnamed: 0']\n",
    "            weight = weight_data.loc[i, 'apoe']\n",
    "            pval = weight_data.loc[i, 'p_apoe']\n",
    "            raw_dx_weight[raw_dx] = [weight, pval]\n",
    "\n",
    "        for concept in concept_iBKH_apoe:\n",
    "            ibkh_name = concept_iBKH_apoe[concept][1]\n",
    "            ibkh_type = concept_iBKH_apoe[concept][2]\n",
    "            if pval_filter:\n",
    "                weight, pval = raw_dx_weight[concept]\n",
    "                if pval <= 0.2:\n",
    "                    input_entity_list.append([ibkh_name, ibkh_type, weight])\n",
    "            else:\n",
    "                weight = raw_dx_weight[concept][0]\n",
    "                input_entity_list.append([ibkh_name, ibkh_type, weight])\n",
    "        if pval_filter:\n",
    "            with open(\"input_entity_list_LR_pval.obj\", \"wb\") as f:\n",
    "                pickle.dump(input_entity_list, f)\n",
    "            f.close()\n",
    "        else:\n",
    "            with open(\"input_entity_list_LR.obj\", \"wb\") as f:\n",
    "                pickle.dump(input_entity_list, f)\n",
    "            f.close()\n",
    "    else:\n",
    "        apoe_proportion = np.asarray(apoe_disease_data['exact_proportion'].tolist())\n",
    "        nonapoe_proportion = np.asarray(nonapoe_disease_data['exact_proportion'].tolist())\n",
    "        raw_weight = apoe_proportion / nonapoe_proportion\n",
    "        log_weight = np.emath.log(raw_weight)\n",
    "        norm_weight = maxabs_scale(log_weight)\n",
    "        for i, raw_dx in enumerate(raw_dx_list):\n",
    "            raw_dx_weight[raw_dx] = norm_weight[i]\n",
    "        for concept in concept_iBKH_apoe:\n",
    "            ibkh_name = concept_iBKH_apoe[concept][1]\n",
    "            ibkh_type = concept_iBKH_apoe[concept][2]\n",
    "            weight = raw_dx_weight[concept][0]\n",
    "            input_entity_list.append([ibkh_name, ibkh_type, weight])\n",
    "\n",
    "        with open(\"input_entity_list.obj\", \"wb\") as f:\n",
    "            pickle.dump(input_entity_list, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_type = 'LR' # A mixed effect logistic regression model was fitted for each trait as a dependent variable and APOE ε4 status as the independent variable to study their statistical associations, adjusted for age, sex, race, and ethnicity as covariates. \n",
    "pval_filter = True # Since we don’t expect to lose the variables with relatively weak signals, we used a P-value < 0.2 for thresholding.\n",
    "get_cohort_context(weight_type, pval_filter, topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03d3d8",
   "metadata": {},
   "source": [
    "### Step 3: Cohort Context Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fe9f8",
   "metadata": {},
   "source": [
    "We then predicted the context entities of the query cohort, given the description entities and their weights in the query cohort. We used the algorithms (Ensemble model, TransE, TransR, ComplEx and DistMult) to calculate the edge scores. And the edge scores indicate the strength of association between candidate entities and built cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predeict_cohort_context(weight_type, pval_filter, topk):\n",
    "        if weight_type == 'LR':\n",
    "        if pval_filter:\n",
    "            with open(\"input_entity_list_LR_pval.obj\", \"rb\") as f:\n",
    "                input_entity_list = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            with open(\"input_entity_list_LR.obj\", \"rb\") as f:\n",
    "                input_entity_list = pickle.load(f)\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(\"input_entity_list.obj\", \"rb\") as f:\n",
    "            input_entity_list = pickle.load(f)\n",
    "        f.close()\n",
    "    print(len(input_entity_list))\n",
    "\n",
    "    input_emb_ids = map_input2embedding_id(input_entity_list)\n",
    "    TransE_res = get_averaged_rank(input_emb_ids, 'TransE')\n",
    "    TransR_res = get_averaged_rank(input_emb_ids, 'TransR')\n",
    "    ComplEx_res = get_averaged_rank(input_emb_ids, 'ComplEx')\n",
    "    DistMult_res = get_averaged_rank(input_emb_ids, 'DistMult')\n",
    "\n",
    "    output_category = ['Gene', 'Pathway', 'Drug', 'Disease', 'Symptom', 'Side_Effect']\n",
    "\n",
    "    if weight_type == \"LR\":\n",
    "        if pval_filter:\n",
    "            root_path = os.path.dirname('apoe_result/LR_pavl_filter/')\n",
    "        else:\n",
    "            root_path = os.path.dirname('apoe_result/LR/')\n",
    "    else:\n",
    "        root_path = os.path.dirname('apoe_result/Norm/')\n",
    "    if topk is None:\n",
    "        res_path = root_path\n",
    "    else:\n",
    "        res_path = root_path + '/top_' + str(topk) + '/'\n",
    "    if not os.path.exists(res_path):\n",
    "        os.makedirs(res_path)\n",
    "    for oc in output_category:\n",
    "        TransE_oc_res = TransE_res[oc]\n",
    "        TransR_oc_res = TransR_res[oc]\n",
    "        ComplEx_oc_res = ComplEx_res[oc]\n",
    "        DistMult_oc_res = DistMult_res[oc]\n",
    "        if bool(TransE_oc_res):\n",
    "            ensemble_oc_res = vote_result(TransE_oc_res, TransR_oc_res, ComplEx_oc_res, DistMult_oc_res)\n",
    "        else:\n",
    "            ensemble_oc_res = {}\n",
    "\n",
    "        res_table = generate_predict_result(ensemble_oc_res, oc, topk)\n",
    "        if len(res_table) > 0:\n",
    "            res_table.to_csv(res_path + \"/\" + oc + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa7c4a0",
   "metadata": {},
   "source": [
    "### Step 4: Generate Network Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae1e1c",
   "metadata": {},
   "source": [
    "To visualize the predicted context entities of the query cohort, we pull shortest paths between each pair of cohort description entity and context entity. We visualized the network structure using Gephi 0.9 (https://gephi.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network_data(weight_type, pavl_filter, target_type, topk):\n",
    "    if weight_type == \"LR\":\n",
    "        if pavl_filter:\n",
    "            with open(\"input_entity_list_LR_pval.obj\", \"rb\") as f:\n",
    "                input_entity_list = pickle.load(f)\n",
    "            f.close()\n",
    "            data_path = 'apoe_result/LR_pavl_filter/'\n",
    "        else:\n",
    "            with open(\"input_entity_list_LR.obj\", \"rb\") as f:\n",
    "                input_entity_list = pickle.load(f)\n",
    "            f.close()\n",
    "            data_path = 'apoe_result/LR/'\n",
    "    else:\n",
    "        with open(\"input_entity_list.obj\", \"rb\") as f:\n",
    "            input_entity_list = pickle.load(f)\n",
    "        f.close()\n",
    "        data_path = 'apoe_result/Norm/'\n",
    "\n",
    "    if topk is None:\n",
    "        predict_res = pd.read_csv(data_path + target_type + \".csv\")\n",
    "    else:\n",
    "        predict_res = pd.read_csv(data_path + \"top_\" + str(topk) + \"/\" + target_type + \".csv\")\n",
    "\n",
    "    candidate_list = predict_res.set_index('name')['type'].to_dict()\n",
    "\n",
    "    triplets_list = []\n",
    "    concepts_list = []\n",
    "    for input_entity in tqdm(input_entity_list):\n",
    "        ibkh_name, ibkh_type, weight = input_entity\n",
    "        concepts_list.append(ibkh_name)\n",
    "        for candidate in tqdm(candidate_list):\n",
    "            if ibkh_name != candidate:\n",
    "                if ibkh_type == 'Gene':\n",
    "                    cypher_statement = \"MATCH (pre:\" + ibkh_type + \" {symbol: \\\"\" + ibkh_name + \"\\\"}), \"\n",
    "                else:\n",
    "                    cypher_statement = \"MATCH (pre:\" + ibkh_type + \" {name: \\\"\" + ibkh_name + \"\\\"}), \"\n",
    "                if candidate_list[candidate] == 'Gene':\n",
    "                    cypher_statement += \"(can:\" + target_type + \" {symbol: \\\"\" + candidate + \"\\\"}), \"\n",
    "                else:\n",
    "                    cypher_statement += \"(can:\" + target_type + \" {name: \\\"\" + candidate + \"\\\"}), \"\n",
    "                cypher_statement += \"path = shortestPath((pre)-[*..15]-(can)) RETURN path LIMIT 5\"\n",
    "                triplets_list += generate_network_triplets(cypher_statement)\n",
    "    network_data = generate_network_data(triplets_list, concepts_list, candidate_list, \"APOE\")\n",
    "    if weight_type == \"LR\":\n",
    "        if pavl_filter:\n",
    "            with open(data_path + \"top_\" + str(topk) + \"/\" + target_type + '_network.obj', 'wb') as f:\n",
    "                pickle.dump(network_data, f)\n",
    "            f.close()\n",
    "        else:\n",
    "            with open(data_path + \"top_\" + str(topk) + \"/\" + target_type + '_network.obj', 'wb') as f:\n",
    "                pickle.dump(network_data, f)\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(data_path + \"top_\" + str(topk) + \"/\" + target_type + '_network.obj', 'wb') as f:\n",
    "            pickle.dump(network_data, f)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
